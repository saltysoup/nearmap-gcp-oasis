apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: pytorch-mnist-cluster
  labels:
    kueue.x-k8s.io/queue-name: dws-local-queue
  annotations:
    provreq.kueue.x-k8s.io/maxRunDurationSeconds: "41600" # Max run time in sec for DWS
spec:
  suspend: false # pods arent scheduled until GPU nodes are provisioned via DWS
  rayVersion: '2.46.0'
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      metadata:
        annotations: 
          gke-gcsfuse/volumes: "true"
          gke-gcsfuse/cpu-limit: "0"
          gke-gcsfuse/memory-limit: "0"
          gke-gcsfuse/ephemeral-storage-limit: "0"
      spec:
        nodeSelector:
          cloud.google.com/gke-nodepool: "default-pool"
        containers:
        - name: ray-head
          image: rayproject/ray:2.46.0-cu128
          ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
          resources:
            limits:
              cpu: "2"
              memory: "5G"
              ephemeral-storage: "9Gi"
            requests:
              cpu: "2"
              memory: "5G"
              ephemeral-storage: "9Gi"
          volumeMounts:
            - name: ray-logs
              mountPath: /tmp/ray
            - name: gcs-fuse-csi-eph
              mountPath: /bucket
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: gcs-fuse-csi-eph
            csi:
              driver: gcsfuse.csi.storage.gke.io
              volumeAttributes:
                bucketName: ikwak-ray # MODIFY: $BUCKET_NAME name of GCS bucket
                gcsfuseMetadataPrefetchOnMount: "true"
                mountOptions: "implicit-dirs,file-mode=777,dir-mode=777,file-cache:enable-parallel-downloads:true,file-cache:cache-file-for-range-read:true,file-cache:max-size-mb:-1,write:enable-streaming-writes:true,read_ahead_kb=1024,file-system:kernel-list-cache-ttl-secs:-1"
  workerGroupSpecs:
  - replicas: 2
    minReplicas: 0
    maxReplicas: 4
    groupName: gpu-group
    rayStartParams:
      num-cpus: "220"
    template:
      metadata:
        annotations:
          gke-gcsfuse/volumes: "true"
          gke-gcsfuse/cpu-limit: "0"
          gke-gcsfuse/memory-limit: "0"
          gke-gcsfuse/ephemeral-storage-limit: "0"
          kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
          networking.gke.io/default-interface: 'eth0'
          networking.gke.io/interfaces: |
            [
              {"interfaceName":"eth0","network":"default"},
              {"interfaceName":"eth1","network":"gvnic-1"},
              {"interfaceName":"eth2","network":"rdma-0"},
              {"interfaceName":"eth3","network":"rdma-1"},
              {"interfaceName":"eth4","network":"rdma-2"},
              {"interfaceName":"eth5","network":"rdma-3"},
              {"interfaceName":"eth6","network":"rdma-4"},
              {"interfaceName":"eth7","network":"rdma-5"},
              {"interfaceName":"eth8","network":"rdma-6"},
              {"interfaceName":"eth9","network":"rdma-7"}
            ]
      spec:
        serviceAccountName: "ksa-ray" # MODIFY: $KSA_NAME name of kubernetes service account
        nodeSelector:
          cloud.google.com/gke-nodepool: "h200-dws"
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        containers:
        - name: ray-worker
          image: rayproject/ray:2.46.0-cu128
          env:
          - name: LD_LIBRARY_PATH
            value: /usr/local/nvidia/lib64
          command: ["source /usr/local/gib/scripts/set_nccl_env.sh && unset NCCL_NET"] #workaround for gibv1.0.6
          resources:
            limits:
              cpu: "220"
              memory: "2800Gi"
              nvidia.com/gpu: "8"
              ephemeral-storage: "1000Gi"
            requests:
              cpu: "220"
              memory: "2800Gi"
              nvidia.com/gpu: "8"
              ephemeral-storage: "1000Gi"
          volumeMounts:
          - name: nvidia
            mountPath: /usr/local/nvidia
          - name: gib
            mountPath: /usr/local/gib
          - name: shared-memory
            mountPath: /dev/shm
          - name: ray-tmp-storage
            mountPath: /tmp
          - name: gcs-fuse-csi-eph
            mountPath: /bucket
        volumes:
        - name: gib
          hostPath:
            path: /home/kubernetes/bin/gib
        - name: nvidia
          hostPath:
            path: /home/kubernetes/bin/nvidia
        - name: lib64
          hostPath:
            path: /lib64
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        - name: sys
          hostPath:
            path: /sys
        - name: proc-sys
          hostPath:
            path: /proc/sys
        - name: ray-tmp-storage
          emptyDir: {}
        - name: gcs-fuse-csi-eph
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: ikwak-ray # MODIFY: $BUCKET_NAME name of GCS bucket
              gcsfuseMetadataPrefetchOnMount: "true"
              mountOptions: "implicit-dirs,file-mode=777,dir-mode=777,file-cache:enable-parallel-downloads:true,file-cache:cache-file-for-range-read:true,file-cache:max-size-mb:-1,write:enable-streaming-writes:true,read_ahead_kb=1024,file-system:kernel-list-cache-ttl-secs:-1"
